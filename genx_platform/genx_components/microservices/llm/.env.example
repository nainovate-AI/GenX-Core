
# Service Configuration
SERVICE_NAME=llm-service
SERVICE_PORT=50053
ENVIRONMENT=development
DEBUG=true

# Model Configuration
DEFAULT_MODEL_ID=gpt2
MODEL_CACHE_DIR=~/.cache/opea/models

# Backend Configuration
# Options: transformers, vllm, mlx, onnx, cloud
# Leave empty for auto-selection based on hardware
BACKEND_TYPE=

# Auto-select best backend based on hardware
AUTO_SELECT_BACKEND=true

# Model Loading Options
TRUST_REMOTE_CODE=false
LOAD_IN_8BIT=false
LOAD_IN_4BIT=false
DEVICE_MAP=auto

# Request Limits
MAX_BATCH_SIZE=32
MAX_CONCURRENT_REQUESTS=10
REQUEST_TIMEOUT_SECONDS=300

# Observability
TELEMETRY_ENABLED=true
TELEMETRY_ENDPOINT=http://localhost:4317
METRICS_PORT=9090

# Service Discovery (optional)
REGISTRY_ENABLED=false
REGISTRY_ENDPOINT=http://localhost:8500