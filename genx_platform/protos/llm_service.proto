syntax = "proto3";

package genx.llm.v1;

option go_package = "github.com/genx/platform/api/llm/v1;llmv1";
option java_multiple_files = true;
option java_package = "com.genx.platform.api.llm.v1";

import "common.proto";
import "google/protobuf/struct.proto";
import "google/protobuf/timestamp.proto";

// LLM service for text generation and model management
service LLMService {
  // Model Management APIs
  rpc LoadModel(LoadModelRequest) returns (LoadModelResponse);
  rpc UnloadModel(UnloadModelRequest) returns (UnloadModelResponse);
  rpc GetLoadedModels(GetLoadedModelsRequest) returns (GetLoadedModelsResponse);
  
  // Text Generation APIs
  rpc GenerateText(GenerateTextRequest) returns (GenerateTextResponse);
  rpc StreamGenerateText(GenerateTextRequest) returns (stream StreamGenerateTextResponse);
  
  // Existing APIs (kept for compatibility)
  rpc Generate(GenerateRequest) returns (GenerateResponse);
  rpc StreamGenerate(GenerateRequest) returns (stream StreamGenerateResponse);
  rpc ListModels(ListModelsRequest) returns (ListModelsResponse);
  rpc GetModel(GetModelRequest) returns (GetModelResponse);
  rpc ValidatePrompt(ValidatePromptRequest) returns (ValidatePromptResponse);
}

// Model Management Messages

message LoadModelRequest {
  // Request metadata
  genx.common.v1.RequestMetadata metadata = 1;
  
  // Model configuration
  string model_name = 2;
  string backend = 3;  // transformers, mlx, vllm, tgi, etc.
  string device = 4;   // cuda, cpu, mps, auto
  
  // Optional backend-specific configuration
  google.protobuf.Struct backend_config = 5;
  
  // Model loading options
  ModelLoadingOptions options = 6;
}

message ModelLoadingOptions {
  // Quantization options
  bool load_in_8bit = 1;
  bool load_in_4bit = 2;
  
  // Memory options
  float gpu_memory_fraction = 3;  // 0.0-1.0
  string device_map = 4;          // auto, balanced, sequential
  
  // Performance options
  int32 tensor_parallel_size = 5;  // For vLLM
  bool use_flash_attention = 6;
  
  // Trust remote code (for custom models)
  bool trust_remote_code = 7;
}

message LoadModelResponse {
  // Response metadata
  genx.common.v1.ResponseMetadata metadata = 1;
  
  // Success status
  bool success = 2;
  
  // Loaded model ID (unique identifier)
  string model_id = 3;
  
  // Model information
  LoadedModelInfo model_info = 4;
  
  // Error details if failed
  optional genx.common.v1.ErrorDetail error = 5;
}

message UnloadModelRequest {
  // Request metadata
  genx.common.v1.RequestMetadata metadata = 1;
  
  // Model to unload
  string model_id = 2;
}

message UnloadModelResponse {
  // Response metadata
  genx.common.v1.ResponseMetadata metadata = 1;
  
  // Success status
  bool success = 2;
  
  // Error details if failed
  optional genx.common.v1.ErrorDetail error = 3;
}

message GetLoadedModelsRequest {
  // Request metadata
  genx.common.v1.RequestMetadata metadata = 1;
  
  // Optional filters
  optional string backend = 2;
  optional string device = 3;
  bool include_stats = 4;
}

message GetLoadedModelsResponse {
  // Response metadata
  genx.common.v1.ResponseMetadata metadata = 1;
  
  // List of loaded models
  repeated LoadedModelInfo models = 2;
  
  // System resource usage
  SystemResourceInfo system_info = 3;
}

message LoadedModelInfo {
  // Unique model ID
  string model_id = 1;
  
  // Model name (e.g., "gpt2", "llama-2-7b")
  string model_name = 2;
  
  // Backend being used
  string backend = 3;
  
  // Device it's loaded on
  string device = 4;
  
  // Loading timestamp
  google.protobuf.Timestamp loaded_at = 5;
  
  // Usage statistics
  ModelUsageStats stats = 6;
  
  // Model capabilities
  ModelCapabilities capabilities = 7;
  
  // Current status
  ModelStatus status = 8;
  
  // Memory usage
  MemoryUsage memory_usage = 9;
}

message ModelUsageStats {
  // Total requests handled
  int64 total_requests = 1;
  
  // Total tokens generated
  int64 total_tokens_generated = 2;
  
  // Average response time (ms)
  double avg_response_time_ms = 3;
  
  // Last used timestamp
  google.protobuf.Timestamp last_used = 4;
}

message ModelCapabilities {
  // Maximum context length
  int32 max_context_length = 1;
  
  // Supported features
  repeated string features = 2;  // e.g., "streaming", "function_calling", "vision"
  
  // Model type
  string model_type = 3;  // e.g., "causal_lm", "seq2seq", "embedding"
}

message MemoryUsage {
  // GPU memory in MB
  int64 gpu_memory_mb = 1;
  
  // RAM in MB
  int64 ram_mb = 2;
  
  // Model size on disk in MB
  int64 disk_size_mb = 3;
}

message SystemResourceInfo {
  // Available GPUs
  repeated GPUInfo gpus = 1;
  
  // CPU info
  CPUInfo cpu = 2;
  
  // Memory info
  SystemMemoryInfo memory = 3;
}

message GPUInfo {
  int32 index = 1;
  string name = 2;
  int64 total_memory_mb = 3;
  int64 used_memory_mb = 4;
  float utilization_percent = 5;
}

message CPUInfo {
  int32 cores = 1;
  float utilization_percent = 2;
}

message SystemMemoryInfo {
  int64 total_ram_mb = 1;
  int64 used_ram_mb = 2;
  int64 available_ram_mb = 3;
}

// Text Generation Messages

message GenerateTextRequest {
  // Request metadata with user info
  genx.common.v1.RequestMetadata metadata = 1;
  
  // User ID for tracking
  string user_id = 2;
  
  // The prompt to generate from
  string prompt = 3;
  
  // Prompt configuration
  PromptConfig prompt_config = 4;
  
  // Whether to include conversation history
  bool use_history = 5;
  
  // Whether to stream the response
  bool streaming = 6;
  
  // Model to use (optional, uses default if not specified)
  optional string model_id = 7;
  
  // Generation parameters
  TextGenerationConfig generation_config = 8;
}

message PromptConfig {
  // System prompt to set context
  optional string system_prompt = 1;
  
  // Additional context or instructions
  optional string context = 2;
  
  // Format instructions
  optional string format_instructions = 3;
  
  // Examples for few-shot learning
  repeated Example examples = 4;
}

message Example {
  string input = 1;
  string output = 2;
}

message TextGenerationConfig {
  // Maximum tokens to generate
  int32 max_tokens = 1;
  
  // Temperature (0.0 - 2.0)
  float temperature = 2;
  
  // Top-p sampling
  float top_p = 3;
  
  // Top-k sampling
  int32 top_k = 4;
  
  // Repetition penalty
  float repetition_penalty = 5;
  
  // Stop sequences
  repeated string stop_sequences = 6;
  
  // Presence penalty (-2.0 to 2.0)
  float presence_penalty = 7;
  
  // Frequency penalty (-2.0 to 2.0)
  float frequency_penalty = 8;
  
  // Random seed for reproducibility
  optional int32 seed = 9;
}

message GenerateTextResponse {
  // Response metadata
  genx.common.v1.ResponseMetadata metadata = 1;
  
  // Generated text
  string generated_text = 2;
  
  // Model used
  string model_id = 3;
  
  // Token usage
  genx.common.v1.TokenUsage usage = 4;
  
  // Generation statistics
  GenerationStats stats = 5;
  
  // Finish reason
  string finish_reason = 6;
  
  // Error if any
  optional genx.common.v1.ErrorDetail error = 7;
}

message StreamGenerateTextResponse {
  // Response metadata (sent in first message)
  optional genx.common.v1.ResponseMetadata metadata = 1;
  
  // Text chunk
  string text_chunk = 2;
  
  // Cumulative text so far
  optional string cumulative_text = 3;
  
  // Is this the final chunk?
  bool is_final = 4;
  
  // Token usage (sent in final message)
  optional genx.common.v1.TokenUsage usage = 5;
  
  // Generation statistics (sent in final message)
  optional GenerationStats stats = 6;
  
  // Finish reason (sent in final message)
  optional string finish_reason = 7;
  
  // Error if any
  optional genx.common.v1.ErrorDetail error = 8;
}

message GenerationStats {
  // Time to first token (ms)
  int64 time_to_first_token_ms = 1;
  
  // Total generation time (ms)
  int64 total_time_ms = 2;
  
  // Tokens per second
  float tokens_per_second = 3;
  
  // Model loading time if applicable (ms)
  optional int64 model_load_time_ms = 4;
}

// Keep existing messages for backward compatibility...
// [Previous GenerationConfig, GenerateRequest, etc. remain the same]